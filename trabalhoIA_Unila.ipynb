{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RomuloAlves2507/TrabalhoIA_unila_/blob/main/trabalhoIA_Unila.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkMXQ9bon-Wi"
      },
      "source": [
        "Trabalho de IA para Engenharia, tem como objetivo desenvolver e avaliar modelos de machine learning para diagnosticar a condição operacional de quatro componentes críticos (resfriador, válvula, bomba e acumulador) de um sistema hidráulico, utilizando dados brutos de múltiplos sensores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed7GYT3tiZ-W"
      },
      "source": [
        "Condition monitoring of hydraulic systems Data Set at ZeMA\n",
        "     https://zenodo.org/records/1323611#.XfzEAEFCeUm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "msxixzo2jqWh"
      },
      "outputs": [],
      "source": [
        "#import das bibliotecas necessárias\n",
        "import pandas as pd\n",
        "import pyarrow\n",
        "from google.colab import files\n",
        "import io\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "EwRvICzEcWY3"
      },
      "outputs": [],
      "source": [
        " #Criar arquivo parket a partir de 18 arquivos .txt (17 reais de sensores e 1 arquivo alvo)\n",
        "def processar_arquivos_sensores(uploaded_files):\n",
        "    print(\"Processando arquivos de SENSORES...\")\n",
        "    lista_de_dados_dos_arquivos = []\n",
        "    numero_de_linhas_esperado = -1\n",
        "\n",
        "    nomes_dos_arquivos = sorted(uploaded_files.keys())\n",
        "\n",
        "    for filename in nomes_dos_arquivos:\n",
        "        content = uploaded_files[filename].decode('utf-8')\n",
        "        linhas = io.StringIO(content).readlines()\n",
        "\n",
        "        if numero_de_linhas_esperado == -1:\n",
        "            numero_de_linhas_esperado = len(linhas)\n",
        "        elif len(linhas) != numero_de_linhas_esperado:\n",
        "            print(f\" ERRO: Arquivos de sensor com números de linhas diferentes!\")\n",
        "            print(f\"Esperado: {numero_de_linhas_esperado}, Arquivo {filename} tem: {len(linhas)}\")\n",
        "            return None\n",
        "\n",
        "    if numero_de_linhas_esperado == 0:\n",
        "        print(\"Aviso: Os arquivos de sensores estão vazios.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Validação OK: Todos os {len(nomes_dos_arquivos)} arquivos de sensor têm {numero_de_linhas_esperado} linhas.\")\n",
        "\n",
        "    dados_combinados_finais = []\n",
        "\n",
        "    for i in range(numero_de_linhas_esperado):\n",
        "        linha_final_combinada = []\n",
        "        for dados_arquivo_em_memoria in lista_de_dados_dos_arquivos:\n",
        "            pass\n",
        "        pass\n",
        "\n",
        "    lista_de_dados_em_memoria = []\n",
        "    for filename in nomes_dos_arquivos:\n",
        "         content = uploaded_files[filename].decode('utf-8')\n",
        "         linhas = io.StringIO(content).readlines()\n",
        "         lista_de_dados_em_memoria.append(linhas)\n",
        "\n",
        "    dados_combinados_finais = []\n",
        "    for i in range(numero_de_linhas_esperado):\n",
        "        linha_final_combinada = []\n",
        "        for dados_arquivo in lista_de_dados_em_memoria:\n",
        "            linha_atual = dados_arquivo[i].strip()\n",
        "            valores_split = linha_atual.split()\n",
        "\n",
        "            try:\n",
        "                valores_float = []\n",
        "                for val in valores_split:\n",
        "                    val_corrigido = val.replace(',', '.')\n",
        "                    valores_float.append(float(val_corrigido))\n",
        "                linha_final_combinada.extend(valores_float)\n",
        "            except ValueError as e:\n",
        "                print(f\" ERRO DE DADOS (Sensor) \")\n",
        "                print(f\"Linha: {i+1}, Valor: '{val}'\")\n",
        "                return None\n",
        "\n",
        "        dados_combinados_finais.append(linha_final_combinada)\n",
        "\n",
        "    print(\"Processamento de sensores concluído.\")\n",
        "    # Renomeia as colunas dos sensores para \"f_0\", \"f_1\", ... (feature_0, feature_1)\n",
        "    df_sensores = pd.DataFrame(dados_combinados_finais)\n",
        "    df_sensores.columns = [f'f_{i}' for i in range(len(df_sensores.columns))]\n",
        "\n",
        "    return df_sensores, numero_de_linhas_esperado\n",
        "\n",
        "def processar_arquivo_profile(uploaded_file, linhas_esperadas):\n",
        "    print(\"Processando arquivo PROFILE...\")\n",
        "\n",
        "    if len(uploaded_file) != 1:\n",
        "        print(\"ERRO: Por favor, selecione exatamente UM arquivo profile.txt\")\n",
        "        return None\n",
        "\n",
        "    filename = list(uploaded_file.keys())[0]\n",
        "    content = uploaded_file[filename].decode('utf-8')\n",
        "    linhas = io.StringIO(content).readlines()\n",
        "\n",
        "    if len(linhas) != linhas_esperadas:\n",
        "        print(f\"ERRO DE SINCRONIZAÇÃO\")\n",
        "        print(f\"Os arquivos de sensor têm {linhas_esperadas} linhas, mas o profile.txt tem {len(linhas)} linhas.\")\n",
        "        return None\n",
        "\n",
        "    print(\"Validação OK: O arquivo profile tem o mesmo número de linhas dos sensores.\")\n",
        "\n",
        "    dados_profile = []\n",
        "    for i, linha_texto in enumerate(linhas):\n",
        "        linha_atual = linha_texto.strip().split()\n",
        "\n",
        "        # O profile.txt tem valores inteiros, não float\n",
        "        try:\n",
        "            # Converte para int\n",
        "            valores_int = [int(float(v.replace(',', '.'))) for v in linha_atual]\n",
        "            dados_profile.append(valores_int)\n",
        "        except ValueError as e:\n",
        "            print(f\"ERRO DE DADOS (Profile)\")\n",
        "            print(f\"Linha: {i+1}, Valor: '{linha_atual}'\")\n",
        "            return None\n",
        "\n",
        "    # Nomes das colunas alvo\n",
        "    colunas_alvo = ['resfriador', 'valvula', 'bomba', 'acumulador', 'stable_flag']\n",
        "\n",
        "    df_profile = pd.DataFrame(dados_profile, columns=colunas_alvo[:len(dados_profile[0])])\n",
        "\n",
        "    # Ajusta os nomes das colunas se o profile tiver menos de 5 colunas\n",
        "    if len(df_profile.columns) < 5:\n",
        "        df_profile.columns = colunas_alvo[:len(df_profile.columns)]\n",
        "\n",
        "    print(\"Processamento do profile concluído.\")\n",
        "    return df_profile\n",
        "\n",
        "def combinar_parquet():\n",
        "    try:\n",
        "        #Carregar Arquivos de Sensores (Features - X) ---\n",
        "        print(\"ETAPA 1: SENSORES (FEATURES)\")\n",
        "        print(\"Por favor, selecione os 17 arquivos .txt dos SENSORES (PS1, TS1, CE, etc.)\")\n",
        "        uploaded_sensores = files.upload()\n",
        "        if not uploaded_sensores:\n",
        "            print(\"Nenhum arquivo de sensor selecionado. Operação cancelada.\")\n",
        "            return\n",
        "\n",
        "        df_X, num_linhas = processar_arquivos_sensores(uploaded_sensores)\n",
        "        if df_X is None:\n",
        "            return\n",
        "\n",
        "        #Carregar Arquivo de Alvo (Target - y)\n",
        "        print(\"\\n ETAPA 2: ALVO (TARGET)\")\n",
        "        print(\"Por favor, selecione o arquivo .txt de ALVO (profile.txt)\")\n",
        "        uploaded_profile = files.upload()\n",
        "        if not uploaded_profile:\n",
        "            print(\"Nenhum arquivo de alvo selecionado. Operação cancelada.\")\n",
        "            return\n",
        "\n",
        "        df_y = processar_arquivo_profile(uploaded_profile, num_linhas)\n",
        "        if df_y is None:\n",
        "            return\n",
        "\n",
        "        print(\"\\n ETAPA 3: COMBINANDO DADOS\")\n",
        "\n",
        "        # Concatena os DataFrames lado a lado (axis=1)\n",
        "        df_final_combinado = pd.concat([df_X, df_y], axis=1)\n",
        "\n",
        "        print(f\"Sucesso! Dados combinados.\")\n",
        "        print(f\"Shape Final: {df_final_combinado.shape}\")\n",
        "        print(\"Primeiras 5 linhas e últimas 5 colunas (para conferir):\")\n",
        "        print(df_final_combinado.iloc[:5, -5:])\n",
        "\n",
        "        #Salvar e Baixar o Parquet\n",
        "        output_filename = 'dados_completos.parquet'\n",
        "        print(f\"\\nSalvando arquivo Parquet como: {output_filename}\")\n",
        "\n",
        "        df_final_combinado.to_parquet(output_filename, engine='pyarrow', index=False)\n",
        "\n",
        "        print(f\"\\n SUCESSO! :) \")\n",
        "        print(f\"Iniciando o download de '{output_filename}' para seu computador...\")\n",
        "        files.download(output_filename)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n OCORREU UM ERRO \")\n",
        "        print(f\"Erro: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "id": "n1EEmSCscbWc"
      },
      "outputs": [],
      "source": [
        "# Configurações Globais\n",
        "TARGET_COLUMNS = ['resfriador', 'valvula', 'bomba', 'acumulador']\n",
        "FLAG_COLUMN = ['stable_flag']\n",
        "X_global = None\n",
        "y_global = None\n",
        "\n",
        "def carregar_arquivo_parquet():\n",
        "    print(\"Abrindo seletor de arquivos... Por favor, selecione seu arquivo .parquet\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"Nenhum arquivo selecionado. Operação cancelada.\")\n",
        "        return None\n",
        "\n",
        "    if len(uploaded) > 1:\n",
        "        print(\"Por favor, selecione apenas UM arquivo. Operação cancelada.\")\n",
        "        return None\n",
        "\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    print(f\"Carregando '{filename}'...\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_parquet(io.BytesIO(uploaded[filename]))\n",
        "        print(\"Arquivo Parquet carregado com sucesso.\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao ler o arquivo Parquet: {e}\")\n",
        "        return None\n",
        "\n",
        "def separar_features_e_alvos(df_completo):\n",
        "    # Separa o DataFrame em X (features) e y (alvos)\n",
        "    if df_completo is None:\n",
        "        print(\"DataFrame de entrada está Vazio. Não é possível separar os dados.\")\n",
        "        return None, None\n",
        "\n",
        "    print(\"Separando features (X) e alvos (y)...\")\n",
        "\n",
        "    try:\n",
        "        y_targets = df_completo[TARGET_COLUMNS]\n",
        "        colunas_para_remover_de_X = TARGET_COLUMNS + FLAG_COLUMN\n",
        "        X_features = df_completo.drop(columns=colunas_para_remover_de_X)\n",
        "\n",
        "        print(f\" Shape das Features (X): {X_features.shape}\")\n",
        "        print(f\" Shape dos Alvos (y):     {y_targets.shape}\")\n",
        "\n",
        "        return X_features, y_targets\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\" ERRO: Coluna não encontrada!\")\n",
        "        print(f\"Verifique as constantes 'TARGET_COLUMNS' e 'FLAG_COLUMN'.\")\n",
        "        print(f\"Colunas encontradas: {list(df_completo.columns)}\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "         print(f\"Erro inesperado ao separar features e alvos: {e}\")\n",
        "         return None, None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "e2yn_7MH2wzW"
      },
      "outputs": [],
      "source": [
        "def treinar_randomForest(X, y):\n",
        "    print(\"--- TREINANDO POR: RANDOM FOREST (BASELINE) ---\")\n",
        "\n",
        "    resultados = {}\n",
        "\n",
        "    for target_name in y.columns:\n",
        "        print(f\"\\nANALISANDO: {target_name.upper()}\")\n",
        "\n",
        "        # Prepara os dados\n",
        "        y_target = y[target_name].astype(str)\n",
        "\n",
        "        # Divisão Treino/Teste (70% treino, 30% teste)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y_target, test_size=0.3, random_state=42, stratify=y_target\n",
        "        )\n",
        "\n",
        "        # Cria e Treina o Modelo (Usando 100 árvores)\n",
        "        clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        # Previsão\n",
        "        y_pred = clf.predict(X_test)\n",
        "\n",
        "        # Métricas\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        resultados[target_name] = acc\n",
        "\n",
        "        print(f\"Acurácia alcançada: {acc*100:.2f}%\")\n",
        "\n",
        "        # Plot Simples da Matriz de Confusão\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        plt.figure(figsize=(5, 4))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')\n",
        "        plt.title(f'Matriz de Confusão: {target_name}')\n",
        "        plt.ylabel('Real')\n",
        "        plt.xlabel('Predito')\n",
        "        plt.show()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RESUMO FINAL DE ACURÁCIA:\")\n",
        "    for componente, acc in resultados.items():\n",
        "        print(f\"{componente.ljust(15)}: {acc*100:.2f}%\")\n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1VcoUk_i7Gxo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2yVZ-vm2wh4"
      },
      "outputs": [],
      "source": [
        "def arvore_decisao(X, y):\n",
        "\n",
        "    print(\"--- TREINAMENTO: ÁRVORE DE DECISÃO (Decision Tree) ---\")\n",
        "\n",
        "    for target_name in y.columns:\n",
        "        print(f\"\\nANALISANDO: {target_name.upper()}\")\n",
        "        y_target = y[target_name].astype(str)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y_target, test_size=0.3, random_state=42, stratify=y_target\n",
        "        )\n",
        "\n",
        "        # Cria o modelo (sem limite de profundidade inicial)\n",
        "        clf = DecisionTreeClassifier(random_state=42)\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        acc = accuracy_score(y_test, clf.predict(X_test))\n",
        "        print(f\"Acurácia: {acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def regressao_logistica(X, y):\n",
        "\n",
        "    print(\"--- TREINAMENTO: REGRESSÃO LOGÍSTICA ---\")\n",
        "\n",
        "    for target_name in y.columns:\n",
        "        print(f\"\\nANALISANDO: {target_name.upper()}\")\n",
        "        y_target = y[target_name].astype(str)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y_target, test_size=0.3, random_state=42, stratify=y_target\n",
        "        )\n",
        "\n",
        "        # Cria Pipeline: Padronização -> Modelo Linear\n",
        "        clf = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000, random_state=42))\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        acc = accuracy_score(y_test, clf.predict(X_test))\n",
        "        print(f\"Acurácia: {acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "S1QFPhBv7HXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b7nktgZo7GfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIp1jJKIwuFf"
      },
      "outputs": [],
      "source": [
        "from numpy import gradient\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    from IPython.display import clear_output\n",
        "\n",
        "    while(True):\n",
        "\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        print(\"--- Análise de Dados Hidráulicos ---\")\n",
        "        print(\"O que você gostaria de fazer?\")\n",
        "        print(\"  [1] Criar um novo arquivo 'dados_completos.parquet' a partir dos arquivos .txt\")\n",
        "        print(\"  [2] Carregar um arquivo .parquet existente para treinar\")\n",
        "        print(\"  [3] Iniciar Treinamento\")\n",
        "        print(\"  [Digite 'sair' para fechar]\")\n",
        "\n",
        "        escolha = input(\"Escolha uma opção: \")\n",
        "\n",
        "        if escolha == '1':\n",
        "            print(\"\\nIniciando Módulo de Criação de Parquet...\")\n",
        "            combinar_parquet()\n",
        "            input(\"\\nProcesso concluído.\")\n",
        "\n",
        "        elif escolha == '2':\n",
        "            print(\"\\nIniciando Carregamento do Arquivo...\")\n",
        "            # 1. Carrega o arquivo Parquet\n",
        "            df_principal = carregar_arquivo_parquet()\n",
        "\n",
        "            if df_principal is not None:\n",
        "                # 2. Separa os dados\n",
        "                X_global, y_global = separar_features_e_alvos(df_principal)\n",
        "\n",
        "                if X_global is not None and y_global is not None:\n",
        "                    print(\"\\n--- Dados prontos para o treinamento ---\")\n",
        "                    pass\n",
        "                else:\n",
        "                    print(\"Falha ao separar features e alvos.\")\n",
        "            else:\n",
        "                print(\"Falha ao carregar o arquivo Parquet.\")\n",
        "\n",
        "            time.sleep(4)\n",
        "\n",
        "        elif escolha == '3':\n",
        "            if X_global is None or y_global is None:\n",
        "                print(\"Nenhum conjunto de dados carregado.\")\n",
        "\n",
        "            else:\n",
        "              print(\"\\nIniciando Treinamento...\")\n",
        "\n",
        "              randomForest(X_global, y_global)\n",
        "\n",
        "              arvore_decisao(X_global, y_global)\n",
        "\n",
        "              regressao_logistica(X_global, y_global)\n",
        "\n",
        "              gradient_boosting(X_global, y_global)\n",
        "\n",
        "              break\n",
        "\n",
        "        elif escolha.lower() == 'sair':\n",
        "            print(\"Encerrando o script.\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"\\nEscolha inválida. Tente novamente.\")\n",
        "            time.sleep(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kv1ux3co2y-6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNq53ba/lzFkJetdbh8l2c0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}